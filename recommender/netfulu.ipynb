{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-e069bfd8a143>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n",
    "\n",
    "# First, generate dictionaries for mapping old id to new id for users and movies\n",
    "unique_MovieID = data_df['MovieID'].unique()\n",
    "unique_UserID = data_df['UserID'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    j += 1\n",
    "j = 0\n",
    "movie_old2new_id_dict = dict()\n",
    "for i in unique_MovieID:\n",
    "    movie_old2new_id_dict[i] = j\n",
    "    j += 1\n",
    "    \n",
    "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
    "user_list = data_df['UserID'].values\n",
    "movie_list = data_df['MovieID'].values\n",
    "for j in range(len(data_df)):\n",
    "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
    "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
    "data_df['UserID'] = user_list\n",
    "data_df['movieID'] = movie_list\n",
    "\n",
    "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
    "train_index = np.random.random(len(data_df)) <= 0.7\n",
    "train_df = data_df[train_index]\n",
    "test_df = data_df[~train_index]\n",
    "\n",
    "# generate train_mat and test_mat\n",
    "num_user = len(data_df['UserID'].unique())\n",
    "num_movie = len(data_df['MovieID'].unique())\n",
    "\n",
    "train_mat = coo_matrix((train_df['Rating'].values, (train_df['UserID'].values, train_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()\n",
    "test_mat = coo_matrix((test_df['Rating'].values, (test_df['UserID'].values, test_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item-Item Collaborative Filtering with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9206510883712525\n",
      "[3577 3609 1805 3500 3629]\n",
      "[5.         5.         5.         5.         5.05876619]\n"
     ]
    }
   ],
   "source": [
    "# implement your improved model and print out the RMSE\n",
    "# Your Code Here...\n",
    "\n",
    "indicator_mat = (train_mat > 0).astype(float)\n",
    "num_rating_items = np.sum(indicator_mat, axis=0, keepdims=True)\n",
    "numer = np.matmul(indicator_mat.T, indicator_mat)  # num_item * num_item\n",
    "denom = num_rating_items.T + num_rating_items - numer  # num_item * num_item\n",
    "denom[denom == 0] = 1\n",
    "J = numer / denom  # num_item * num_item\n",
    "\n",
    "prediction_mat = train_mat.copy()\n",
    "\n",
    "num_rating_items[num_rating_items == 0] = 1\n",
    "mu_items = np.sum(train_mat, axis=0, keepdims=True) / num_rating_items  # 1 * num_item\n",
    "deviation_mat = (train_mat - mu_items) * indicator_mat\n",
    "for i in range(num_movie):\n",
    "    similarities = J[i, :]\n",
    "    similarities[i] = -1\n",
    "    N_idx = np.argpartition(similarities, -10)[-10:]\n",
    "    N_sim = similarities[N_idx]\n",
    "    prediction_mat[:, i] = np.sum(N_sim.reshape((1, -1)) * deviation_mat[:, N_idx], axis=1) / (np.sum(N_sim) + 1e-10)\n",
    "prediction_mat += mu_items\n",
    "\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "print(test_rmse)\n",
    "print(np.argsort(prediction_mat[0, :])[-5:])\n",
    "print(prediction_mat[0, np.argsort(prediction_mat[0, :])[-5:]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
