{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 489 :: Recommender Systems :: Texas A&M University :: Spring 2021\n",
    "\n",
    "\n",
    "# Homework 2: Collaborative Filtering\n",
    "\n",
    "### 100 points [10% of your final grade]\n",
    "\n",
    "- **Due Friday, February 26 by 11:59pm**\n",
    "\n",
    "*Goals of this homework:* The objective of this homework is to turn the theory of collaborative filtering into practice, and compare the results with your naive non-personalized recommendation from Homework 1 to see how personalized collaborative filtering algorithms provide better recommendations for both the rating prediction task (with explicit feedback) and the ranking task (with implicit feedback).\n",
    "\n",
    "*Submission instructions (Canvas):* To submit your homework, rename this notebook as `UIN_hw2.ipynb`. For example, if your UIN is `123456789`, then your homework submission would be `123456789_hw2.ipynb`. Submit this notebook via Canvas. Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit. \n",
    "\n",
    "*Late policy:* You may use up to three of your late days on this assignment. No homeworks will be accepted after March 1 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration Declaration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***You must add all of your collaboration declarations here. Who did you talk to about this assignment? What web resources did you use? Etc.***\n",
    "\n",
    "For example:\n",
    "* Part 1a: I talked to Amy about how to split the data randomly. She helped me understand that I needed to use a random number generator.\n",
    "* Part 1b: I needed help on how to comment my code, so I relied on this StackOverflow thread: https://stackoverflow.com/questions/184618/what-is-the-best-comment-in-source-code-you-have-ever-encountered\n",
    "* (Replace this bullet list with your own collaboration declarations.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Recommendations with User Ratings (Explicit Feedback) (60 points total)\n",
    "\n",
    "In this first part, similar to Part 1 in our Homework 1, we still focus on the rating prediction recommendation task with explicit feedback. But in this part, you will need to build **personalized** models instead of non-personalized models as in Homework 1. You will also evaluate your personalized models to compare them with the non-personalized one in Homework 1.\n",
    "\n",
    "For this part, we will:\n",
    "\n",
    "* load and process the MovieLens 1M dataset, \n",
    "* build a baseline estimation model,\n",
    "* build a user-user collaborative filtering model,\n",
    "* improve the user-user collaborative filtering model, and\n",
    "* evaluate and compare these different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start to build your recommendation models. First, we need to load and preprocess the experiment dataset. We still use the MovieLens 1M data from https://grouplens.org/datasets/movielens/1m/ in this homework, and use the same loading and preprocessing process as Homework 1 Part 1a. The code has been provided in the next cell, and you need to run it. The resulting data variables are the same as those in Homework 1: train_mat is the numpy array variable for training data of size (#users, #items) with non-zero entries representing user-item ratings, and zero entries representing unknown user-item ratings; and test_mat is the numpy array variable for testing data of size (#users, #items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-e069bfd8a143>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n",
    "\n",
    "# First, generate dictionaries for mapping old id to new id for users and movies\n",
    "unique_MovieID = data_df['MovieID'].unique()\n",
    "unique_UserID = data_df['UserID'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    j += 1\n",
    "j = 0\n",
    "movie_old2new_id_dict = dict()\n",
    "for i in unique_MovieID:\n",
    "    movie_old2new_id_dict[i] = j\n",
    "    j += 1\n",
    "    \n",
    "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
    "user_list = data_df['UserID'].values\n",
    "movie_list = data_df['MovieID'].values\n",
    "for j in range(len(data_df)):\n",
    "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
    "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
    "data_df['UserID'] = user_list\n",
    "data_df['movieID'] = movie_list\n",
    "\n",
    "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
    "train_index = np.random.random(len(data_df)) <= 0.7\n",
    "train_df = data_df[train_index]\n",
    "test_df = data_df[~train_index]\n",
    "\n",
    "# generate train_mat and test_mat\n",
    "num_user = len(data_df['UserID'].unique())\n",
    "num_movie = len(data_df['MovieID'].unique())\n",
    "\n",
    "train_mat = coo_matrix((train_df['Rating'].values, (train_df['UserID'].values, train_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()\n",
    "test_mat = coo_matrix((test_df['Rating'].values, (test_df['UserID'].values, test_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1a: Build the Baseline Estimation Model (20 points)\n",
    "\n",
    "First, let's implement a simple personalized recommendation model -- the baseline estimate -- introduced in class: $b_{u,i}=\\mu+b_i+b_u$, where $\\mu$ is the overall mean rating for all items, $b_u$ = average rating of user $u-\\mu$, $b_i$ = average rating of item $i-\\mu$. Store your prediction as a numpy array variable 'prediction_mat' of size (#users, #movies) with each entry showing the predicted rating for the corresponding user-movie pair.\n",
    "\n",
    "* Hint: for users who do not have ratings in train_mat, set $b_u=0$ for them; and for movies which do not have ratings in train_mat, set $b_i=0$ for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction_mat by the baseline estimation recommendation algorithm\n",
    "# Your Code Here...\n",
    "\n",
    "indicator_mat = (train_mat > 0).astype(float)\n",
    "mu = np.sum(train_mat) / np.sum(indicator_mat)\n",
    "\n",
    "num_rating_items = np.sum(indicator_mat, axis=0, keepdims=True)\n",
    "num_rating_items[num_rating_items == 0] = 1\n",
    "mu_items = np.sum(train_mat, axis=0, keepdims=True) / num_rating_items\n",
    "mu_items[mu_items == 0] = mu\n",
    "\n",
    "num_rating_users = np.sum(indicator_mat, axis=1, keepdims=True)\n",
    "num_rating_users[num_rating_users == 0] = 1\n",
    "mu_users = np.sum(train_mat, axis=1, keepdims=True) / num_rating_users\n",
    "mu_users[mu_users == 0] = mu\n",
    "bi = mu_items - mu\n",
    "bu = mu_users - mu\n",
    "prediction_mat = np.ones_like(train_mat)\n",
    "prediction_mat = mu + bi + bu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this prediction_mat based on the baseline estimate, let's use the same RMSE metric as you used in Homework 1 Part 1c to evaluate the quality of the baseline estimate model. Please print out the RMSE of your prediction_mat using test_mat in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# calculate and print out the RMSE for your prediction_df and the test_df\n",
    "# Your Code Here...\n",
    "\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you observe an increase or decrease of the RMSE compared with the result you got from Homework 1 Part 1c with a non-personalized recommendation model? What do you think leads to this difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*\n",
    "\n",
    "Yes. Because the baseline estimation consider both user and item baselines togather but non-personalized recommendation model only considers item average rating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: User-user Collaborative Filtering with Jaccard Similarity (30 points)\n",
    "\n",
    "In this part, you need to build a user-user collaborative filtering recommendation model with **Jaccard similarity** to predict user-movie ratings. \n",
    "\n",
    "The prediction of the score for a user-item pair $(u,i)$ should use the formulation: $p_{u,i}=\\bar{r}_u+\\frac{\\sum_{u^\\prime\\in N}s(u,u^\\prime)(r_{u^\\prime,i}-\\bar{r}_{u^\\prime})}{\\sum_{u^\\prime\\in N}|s(u, u^\\prime)|}$ as introduced in class, where $s(u, u^\\prime)$ is the Jaccard similarity. We set the size of $N$ as 10.\n",
    "\n",
    "In the next cell, you need to write your code to implement this algorithm, and generate a numpy array variable named 'prediction_mat' of size (#user, #movie) with each entry showing the predicted rating for the corresponding user-movie pair.\n",
    "\n",
    "* Hint: when you find the nearest neighbor set $N$ of a user $u$, do not include user $u$ in $N$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction_mat by your user-user collaborative filtering recommendation algorithm\n",
    "# Your Code Here...\n",
    "\n",
    "# binary matrix to indicate whether there is a rating for a user-movie pair\n",
    "indicator_mat = (train_mat > 0).astype(float)  # size = (#user, #movie)  \n",
    "\n",
    "# calculate the number of ratings for each user\n",
    "num_rating_per_user = np.sum(indicator_mat, axis=1, keepdims=True)  # size = (#user, 1)  \n",
    "\n",
    "# calculate the numerator of Jaccard similarity: for two users, calculate the number of movies both of they rated\n",
    "numerator = np.matmul(indicator_mat, indicator_mat.T)  # size = (#user, #user)\n",
    "\n",
    "# calculate the denominator of Jaccard similarity: for two users, calculate the number of movies they rated in total\n",
    "denominator = num_rating_per_user + num_rating_per_user.T - numerator  # size = (#user, #user)\n",
    "\n",
    "# set 0 to be 1 to avoid error in division \n",
    "denominator[denominator == 0] = 1\n",
    "\n",
    "# calculate Jaccard similarity matrix\n",
    "Jaccard_mat = numerator / denominator  # size = (#user, #user)\n",
    "\n",
    "prediction_mat = train_mat.copy()\n",
    "\n",
    "num_rating_users[num_rating_users == 0] = 1\n",
    "mu_users = np.sum(train_mat, axis=1, keepdims=True) / num_rating_users\n",
    "deviation_mat = (train_mat - mu_users) * indicator_mat\n",
    "for u in range(num_user):\n",
    "    similarities = Jaccard_mat[u, :]\n",
    "    similarities[u] = -1\n",
    "    N_idx = np.argpartition(similarities, -10)[-10:]\n",
    "    N_sim = similarities[N_idx]\n",
    "    prediction_mat[u, :] = np.sum(N_sim.reshape((-1, 1)) * deviation_mat[N_idx, :], axis=0) / np.sum(N_sim)\n",
    "prediction_mat += mu_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please print out the RMSE of your prediction_mat using test_mat in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824264022914787\n"
     ]
    }
   ],
   "source": [
    "# calculate and print out the RMSE for your prediction_df and the test_df\n",
    "# Your Code Here...\n",
    "\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the RMSE results of this user-user collaborative filtering, the baseline estimate algorithm, and your non-personalized recommendation model from Homework 1 Part 1c, what do you observe? Is the user-user collaborative filtering the one producing the best performance? What reasons do you think can explain what you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here...*\n",
    "\n",
    "No, user-user CF does not perform as well as the baseline estimation model. A possible reason may be in this data, the user-user similarity is not informative enough for accurate recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1c: Improve the Collaborative Filtering Model (20 points)\n",
    "\n",
    "Next, you need to try your best to improve the collaborative filtering model so that we can improve our RMSE! This is open-ended, so feel free to try whatever collaborative filtering tricks you like. We talked about several in class, plus you can find more in the readings. As a starting point, here are some possibilities:\n",
    "\n",
    "* Try the cosine similarity or pearson similarity measurement instead of Jaccard\n",
    "* Try item-item collaborative filtering instead of user-user CF\n",
    "* Try to include the baseline estimation model in your collaborative filtering model\n",
    "\n",
    "Besides these, you can also figure out your own solutions how to improve the performance. Write your code in the next cell and print out the RMSE of your new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013411736923351136\n"
     ]
    }
   ],
   "source": [
    "# implement your improved model and print out the RMSE\n",
    "# Your Code Here...\n",
    "\n",
    "indicator_mat = (train_mat > 0).astype(float)\n",
    "num_rating_items = np.sum(indicator_mat, axis=0, keepdims=True)\n",
    "numer = np.matmul(indicator_mat.T, indicator_mat)  # num_item * num_item\n",
    "denom = num_rating_items.T + num_rating_items - numer  # num_item * num_item\n",
    "denom[denom == 0] = 1\n",
    "J = numer / denom  # num_item * num_item\n",
    "\n",
    "prediction_mat = train_mat.copy()\n",
    "\n",
    "num_rating_items[num_rating_items == 0] = 1\n",
    "mu_items = np.sum(train_mat, axis=0, keepdims=True) / num_rating_items  # 1 * num_item\n",
    "deviation_mat = (train_mat - mu_items) * indicator_mat\n",
    "for i in range(num_movie):\n",
    "    similarities = J[i, :]\n",
    "    similarities[i] = -1\n",
    "    N_idx = np.argpartition(similarities, -10)[-10:]\n",
    "    N_sim = similarities[N_idx]\n",
    "    prediction_mat[:, i] = np.sum(N_sim.reshape((1, -1)) * deviation_mat[:, N_idx], axis=1) / (np.sum(N_sim) + 1e-10)\n",
    "prediction_mat += mu_items\n",
    "\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And please briefly explain what your new model does to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Briefly explain what you do in your new model to improve the performance.\n",
    "Write your answer here...*\n",
    "\n",
    "Use item-item CF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Recommendations with implicit feedback (40 points total)\n",
    "\n",
    "Now we turn to study how collaborative filtering algorithms work for recommendations with implicit feedback. The overall pipeline is the same as in Part 2 of Homework 1. But now you need to implement a user-user collaborative filtering algorithm for recommendation.\n",
    "\n",
    "In this part, you will use the same MovieLens 1M dataset, and:\n",
    "* write the code to implement a user-user collaborative filtering algorithm,\n",
    "* and evaluate your recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the algorithm implementation, we need to first transfer the explicit datasets you already generated to implicit ones. The process is the same as Part 2a in Homework 1, and the code has been provided. You need to run the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = (train_mat > 0).astype(float)\n",
    "test_mat = (test_mat > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to write code to implement a user-user collaborative filtering algorithm with **cosine similarity**. Although we have not discussed in class how to use collaborative filtering for implicit feedback, it is quite straightforward. \n",
    "\n",
    "* We first need to calculate the cosine similarity between users based on the binary feedback vectors of users; \n",
    "* Then, for a specific user $u$, we predict a preference vector for the user $u$ by weighted averaging the binary feedback vectors of N users who are most similar to $u$;\n",
    "* And last, rank the movies based on the predicted preference vector of the user $u$ as recommendations. \n",
    "\n",
    "The predicted preference score from user $u$ to movie $i$ can be calculated as: $p_{u,i}=\\frac{\\sum_{u^\\prime\\in N}s(u,u^\\prime)r_{u^\\prime,i}}{\\sum_{u^\\prime\\in N}|s(u,u^\\prime)|}$, where $s(u,u^\\prime)$ is the cosine similarity, and we set the size of $N$ as 10.\n",
    "\n",
    "In the next cell, write your code to generate the ranked lists of movies by this user-user collaborative filtering recommendation algorithm for every user, store the result in a numpy array of size (#user, 50), where entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. \n",
    "\n",
    "* Hint: for a user $u$, the movies user $u$ liked in the train_mat should be excluded in the top 50 recommendation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "\n",
    "user_train_like = []\n",
    "for u in range(num_user):\n",
    "    user_train_like.append(np.where(train_mat[u, :] > 0)[0])\n",
    "\n",
    "numer = np.matmul(train_mat, train_mat.T)\n",
    "denom = np.sum(train_mat ** 2, axis=1, keepdims=True) ** 0.5\n",
    "Cosine = numer / np.matmul(denom, denom.T)\n",
    "\n",
    "recommendation = []\n",
    "for u in range(num_user):\n",
    "    similarities = Cosine[u, :]\n",
    "    similarities[u] = -1\n",
    "    N_idx = np.argpartition(similarities, -10)[-10:]\n",
    "    N_sim = similarities[N_idx]\n",
    "    scores = np.sum(N_sim.reshape((-1, 1)) * train_mat[N_idx, :], axis=0) / np.sum(N_sim)\n",
    "    \n",
    "    train_like = user_train_like[u]\n",
    "    scores[train_like] = -9999\n",
    "    top50_iid = np.argpartition(scores, -50)[-50:]\n",
    "    top50_iid = top50_iid[np.argsort(scores[top50_iid])[-1::-1]]\n",
    "    recommendation.append(top50_iid)\n",
    "recommendation = np.array(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, you need to evaluate your user-user collaborative filtering algorithm by the held-out testing dataset test_mat for each user. Here, we use the same metrics recall@k and precision@k we used in Part 2c of Homework 1. So you can use the same code here to calculate recall@k and precision@k for k=5, 20, 50 for each user, i.e., six metrics for every user. And please print out the average over all users for these six metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5\t[0.070814],\t||\t recall@20\t[0.191537],\t||\t recall@50\t[0.319397]\n",
      "precision@5\t[0.380430],\t||\t precision@20\t[0.291598],\t||\t precision@50\t[0.221063]\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall@k, precision@k with k=5, 20, 50 and print out the average over all users for these 6 metrics.\n",
    "# Your Code Here...\n",
    "\n",
    "user_test_like = []\n",
    "for u in range(num_user):\n",
    "    user_test_like.append(np.where(test_mat[u, :] > 0)[0])\n",
    "    \n",
    "recalls = np.zeros(3)\n",
    "precisions = np.zeros(3)\n",
    "user_count = 0.\n",
    "\n",
    "for u in range(num_user):\n",
    "    test_like = user_test_like[u]\n",
    "    test_like_num = len(test_like)\n",
    "    if test_like_num == 0:\n",
    "        continue\n",
    "    rec = recommendation[u, :]\n",
    "    hits = np.zeros(3)\n",
    "    for k in range(50):\n",
    "        if rec[k] in test_like:\n",
    "            if k < 50:\n",
    "                hits[2] += 1\n",
    "                if k < 20:\n",
    "                    hits[1] += 1\n",
    "                    if k < 5:\n",
    "                        hits[0] += 1\n",
    "    recalls[0] += (hits[0] / test_like_num)\n",
    "    recalls[1] += (hits[1] / test_like_num)\n",
    "    recalls[2] += (hits[2] / test_like_num)\n",
    "    precisions[0] += (hits[0] / 5.)\n",
    "    precisions[1] += (hits[1] / 20.)\n",
    "    precisions[2] += (hits[2] / 50.)\n",
    "    user_count += 1\n",
    "\n",
    "recalls /= user_count\n",
    "precisions /= user_count\n",
    "\n",
    "print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
    "print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
