{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 489 :: Recommender Systems :: Texas A&M University :: Spring 2021\n",
    "\n",
    "\n",
    "# Homework 2: Collaborative Filtering\n",
    "\n",
    "### 100 points [10% of your final grade]\n",
    "\n",
    "- **Due Friday, February 26 by 11:59pm**\n",
    "\n",
    "*Goals of this homework:* The objective of this homework is to turn the theory of collaborative filtering into practice, and compare the results with your naive non-personalized recommendation from Homework 1 to see how personalized collaborative filtering algorithms provide better recommendations for both the rating prediction task (with explicit feedback) and the ranking task (with implicit feedback).\n",
    "\n",
    "*Submission instructions (Canvas):* To submit your homework, rename this notebook as `UIN_hw2.ipynb`. For example, if your UIN is `123456789`, then your homework submission would be `123456789_hw2.ipynb`. Submit this notebook via Canvas. Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit. \n",
    "\n",
    "*Late policy:* You may use up to three of your late days on this assignment. No homeworks will be accepted after March 1 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration Declaration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***You must add all of your collaboration declarations here. Who did you talk to about this assignment? What web resources did you use? Etc.***\n",
    "\n",
    "For example:\n",
    "* Part 1b: I consulted the Teams channel on how to build the jaccard matrix and how to best deal with missing ratings\n",
    "* referenced the NumPy api docs quite a lot since I don't have any experience with it\n",
    "* Part 1c - I consulted this stack overflow thread to help calculate cosine similarity \n",
    "https://stackoverflow.com/questions/41905029/create-cosine-similarity-matrix-numpy\n",
    "* I needed help figuring out how to get the top 10 nearest neighbors so i consulted this stack overflow thread on partitioning\n",
    "https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Recommendations with User Ratings (Explicit Feedback) (60 points total)\n",
    "\n",
    "In this first part, similar to Part 1 in our Homework 1, we still focus on the rating prediction recommendation task with explicit feedback. But in this part, you will need to build **personalized** models instead of non-personalized models as in Homework 1. You will also evaluate your personalized models to compare them with the non-personalized one in Homework 1.\n",
    "\n",
    "For this part, we will:\n",
    "\n",
    "* load and process the MovieLens 1M dataset, \n",
    "* build a baseline estimation model,\n",
    "* build a user-user collaborative filtering model,\n",
    "* improve the user-user collaborative filtering model, and\n",
    "* evaluate and compare these different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start to build your recommendation models. First, we need to load and preprocess the experiment dataset. We still use the MovieLens 1M data from https://grouplens.org/datasets/movielens/1m/ in this homework, and use the same loading and preprocessing process as Homework 1 Part 1a. The code has been provided in the next cell, and you need to run it. The resulting data variables are the same as those in Homework 1: train_mat is the numpy array variable for training data of size (#users, #items) with non-zero entries representing user-item ratings, and zero entries representing unknown user-item ratings; and test_mat is the numpy array variable for testing data of size (#users, #items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], engine='python')\n",
    "\n",
    "# First, generate dictionaries for mapping old id to new id for users and movies\n",
    "unique_MovieID = data_df['MovieID'].unique()\n",
    "unique_UserID = data_df['UserID'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    j += 1\n",
    "j = 0\n",
    "movie_old2new_id_dict = dict()\n",
    "for i in unique_MovieID:\n",
    "    movie_old2new_id_dict[i] = j\n",
    "    j += 1\n",
    "    \n",
    "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
    "user_list = data_df['UserID'].values\n",
    "movie_list = data_df['MovieID'].values\n",
    "for j in range(len(data_df)):\n",
    "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
    "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
    "data_df['UserID'] = user_list\n",
    "data_df['movieID'] = movie_list\n",
    "\n",
    "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
    "train_index = np.random.random(len(data_df)) <= 0.7\n",
    "train_df = data_df[train_index]\n",
    "test_df = data_df[~train_index]\n",
    "\n",
    "# generate train_mat and test_mat\n",
    "num_user = len(data_df['UserID'].unique())\n",
    "num_movie = len(data_df['MovieID'].unique())\n",
    "\n",
    "\n",
    "train_mat = coo_matrix((train_df['Rating'].values, (train_df['UserID'].values, train_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()\n",
    "test_mat = coo_matrix((test_df['Rating'].values, (test_df['UserID'].values, test_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1a: Build the Baseline Estimation Model (20 points)\n",
    "\n",
    "First, let's implement a simple personalized recommendation model -- the baseline estimate -- introduced in class: $b_{u,i}=\\mu+b_i+b_u$, where $\\mu$ is the overall mean rating for all items, $b_u$ = average rating of user $u-\\mu$, $b_i$ = average rating of item $i-\\mu$. Store your prediction as a numpy array variable 'prediction_mat' of size (#users, #movies) with each entry showing the predicted rating for the corresponding user-movie pair.\n",
    "\n",
    "* Hint: for users who do not have ratings in train_mat, set $b_u=0$ for them; and for movies which do not have ratings in train_mat, set $b_i=0$ for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9db63bc87a43>:24: RuntimeWarning: Mean of empty slice\n",
      "  curr_movie_mean = np.nanmean(prediction_mat[:,i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.9181489  3.98827012 4.70249492 ... 1.53918396 5.53918396 4.53918396]\n",
      " [4.57715656 3.64727778 4.36150258 ... 1.19819162 5.19819162 4.19819162]\n",
      " [4.71360344 3.78372466 4.49794946 ... 1.3346385  5.3346385  4.3346385 ]\n",
      " ...\n",
      " [4.96360344 4.03372466 4.74794946 ... 1.5846385  5.5846385  4.5846385 ]\n",
      " [4.63693678 3.707058   4.4212828  ... 1.25797184 5.25797184 4.25797184]\n",
      " [4.42036774 3.49048896 4.20471376 ... 1.0414028  5.0414028  4.0414028 ]]\n"
     ]
    }
   ],
   "source": [
    "# calculate the prediction_mat by the baseline estimation recommendation algorithm\n",
    "# Your Code Here...\n",
    "\n",
    "def get_baseline_estimate(train_mat):\n",
    "\n",
    "    prediction_mat = train_mat.copy()\n",
    "    prediction_mat[prediction_mat == 0] = np.nan\n",
    "    mu = np.nanmean(prediction_mat)\n",
    "\n",
    "    num_rows = prediction_mat.shape[0]\n",
    "    num_columns = prediction_mat.shape[1]\n",
    "\n",
    "    # looping through each user to get it's bi\n",
    "    curr_user_mean = 0.0\n",
    "    bu = []\n",
    "    for i in range(num_rows):\n",
    "        curr_user_mean = np.nanmean(prediction_mat[i,:])\n",
    "        bu.append(curr_user_mean - mu )\n",
    "\n",
    "    # looping through each column to get movie rating deviation\n",
    "    curr_movie_mean = 0.0\n",
    "    bi = []\n",
    "    for i in range(num_columns):\n",
    "        curr_movie_mean = np.nanmean(prediction_mat[:,i])\n",
    "        bi.append(curr_movie_mean - mu)\n",
    "\n",
    "\n",
    "\n",
    "    bu = np.nan_to_num(bu)\n",
    "    bi = np.nan_to_num(bi)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_columns):\n",
    "            prediction_mat[i][j] = bu[i] + bi[j] + mu\n",
    "    \n",
    "    prediction_mat = np.nan_to_num(prediction_mat)\n",
    "    return prediction_mat\n",
    "\n",
    "\n",
    "prediction_mat = get_baseline_estimate(train_mat)\n",
    "print(prediction_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this prediction_mat based on the baseline estimate, let's use the same RMSE metric as you used in Homework 1 Part 1c to evaluate the quality of the baseline estimate model. Please print out the RMSE of your prediction_mat using test_mat in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9372309125262424\n"
     ]
    }
   ],
   "source": [
    "# calculate and print out the RMSE for your prediction_df and the test_df\n",
    "# Your Code Here...\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you observe an increase or decrease of the RMSE compared with the result you got from Homework 1 Part 1c with a non-personalized recommendation model? What do you think leads to this difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saw a slight decrease compared to hw 1, meaning the prediciton is more accurate. This is expected since people are getting a personalized recommendation. This normalizes someone's rating and takes into account people who give all 4 or 5 star ratings vs. people who tend to give lower ratings for a movie. Taking into account the user's and movie's average mean in the baseline estimate gives us a more realistic predicted rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: User-user Collaborative Filtering with Jaccard Similarity (30 points)\n",
    "\n",
    "In this part, you need to build a user-user collaborative filtering recommendation model with **Jaccard similarity** to predict user-movie ratings. \n",
    "\n",
    "The prediction of the score for a user-item pair $(u,i)$ should use the formulation: $p_{u,i}=\\bar{r}_u+\\frac{\\sum_{u^\\prime\\in N}s(u,u^\\prime)(r_{u^\\prime,i}-\\bar{r}_{u^\\prime})}{\\sum_{u^\\prime\\in N}|s(u, u^\\prime)|}$ as introduced in class, where $s(u, u^\\prime)$ is the Jaccard similarity. We set the size of $N$ as 10.\n",
    "\n",
    "In the next cell, you need to write your code to implement this algorithm, and generate a numpy array variable named 'prediction_mat' of size (#user, #movie) with each entry showing the predicted rating for the corresponding user-movie pair.\n",
    "\n",
    "* Hint: when you find the nearest neighbor set $N$ of a user $u$, do not include user $u$ in $N$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4299 5342 1656 5189   79 5835  478 2500 1480 2649]\n",
      "[3587 4140   12  809 3994   94  102  299 2813 4600]\n"
     ]
    }
   ],
   "source": [
    "# calculate the prediction_mat by your user-user collaborative filtering recommendation algorithm\n",
    "# Your Code Here...\n",
    "indicator_mat = (train_mat > 0).astype(float)\n",
    "\n",
    "# gets array of the # of ratings for each user\n",
    "num_ratings_per_user = np.sum(indicator_mat, axis=1,keepdims=True)\n",
    "\n",
    "# returns array of the # of movies they both rated\n",
    "numerator = np.matmul(indicator_mat, indicator_mat.T) # size = (#users, #users)\n",
    "\n",
    "# get the # of movies both users both rated in total\n",
    "denominator = num_ratings_per_user + num_ratings_per_user.T - numerator # size = (#users, #users)\n",
    "\n",
    "\n",
    "#set 0 to be 1 to avoid error in division\n",
    "denominator[denominator == 0] = 1\n",
    "\n",
    "jaccard_matrix = numerator / denominator\n",
    "\n",
    "#print (jaccard_matrix)\n",
    "\n",
    "num_users = jaccard_matrix.shape[0]\n",
    "kNearestNeighbors = []\n",
    "for i in range(num_users):\n",
    "    curr_users_neighbors = np.argpartition(jaccard_matrix[i], -11)[-11:] # getting 11 bc it will include the curr user\n",
    "    curr_users_neighbors = curr_users_neighbors[curr_users_neighbors != i] # removing user himself from neighbors\n",
    "    kNearestNeighbors.append(curr_users_neighbors)\n",
    "\n",
    "\n",
    "print (kNearestNeighbors[0])\n",
    "print (kNearestNeighbors[1])\n",
    "# print(jaccard_matrix[0])\n",
    "prediction_mat = train_mat.copy()\n",
    "nan_prediction_mat = train_mat.copy() # creating a copy so it's easier to do mean calculations\n",
    "nan_prediction_mat[nan_prediction_mat == 0] = np.nan\n",
    "      \n",
    "for i in range(num_users):\n",
    "    curr_users_mean = np.nanmean(nan_prediction_mat[i])\n",
    "    numerator = 0\n",
    "    denom = 0\n",
    "\n",
    "    for j in range(10):\n",
    "        curr_neighbor = kNearestNeighbors[i][j]\n",
    "        curr_neighbor_rating_vec = prediction_mat[curr_neighbor] # gets row of neighbor's ratinggi\n",
    "\n",
    "        nan_curr_neighbor_rating_vec = nan_prediction_mat[curr_neighbor]\n",
    "        curr_neighbors_mean = np.nanmean(curr_neighbor_rating_vec)\n",
    "        if(curr_neighbors_mean == 0):\n",
    "            continue\n",
    "        similarity = jaccard_matrix[i][kNearestNeighbors[i][j]] # getting the similarity of each of the neighbors\n",
    "        \n",
    "        curr_neighbor_rating_vec[curr_neighbor_rating_vec == 0] = curr_users_mean\n",
    "        rating_dev = curr_neighbor_rating_vec - curr_neighbors_mean\n",
    "        rating_dev *= indicator_mat[curr_neighbor]\n",
    "        numerator += similarity * rating_dev # sets instances where neighbor didn't give a rating to zero\n",
    "        denom += similarity\n",
    "    \n",
    "    prediction_mat[i] = curr_users_mean +  (numerator / denom)\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please print out the RMSE of your prediction_mat using test_mat in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0188813316801908\n"
     ]
    }
   ],
   "source": [
    "# calculate and print out the RMSE for your prediction_df and the test_df\n",
    "# Your Code Here...\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the RMSE results of this user-user collaborative filtering, the baseline estimate algorithm, and your non-personalized recommendation model from Homework 1 Part 1c, what do you observe? Is the user-user collaborative filtering the one producing the best performance? What reasons do you think can explain what you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's slightly worse than the baseline estimate and slightly worse than the non-personalized recommendation model in HW1 Part c. I don't think it's the nature of user-user collaborative filtering that is making this worse necessarily, but it's because the jaccard matrix doesn't take into account actual ratings when determining how similar two users are. So it seems that non-personalized and the baseline recommenders are better than a personalized recommender when the similarity rating between user is not strong enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1c: Improve the Collaborative Filtering Model (20 points)\n",
    "\n",
    "Next, you need to try your best to improve the collaborative filtering model so that we can improve our RMSE! This is open-ended, so feel free to try whatever collaborative filtering tricks you like. We talked about several in class, plus you can find more in the readings. As a starting point, here are some possibilities:\n",
    "\n",
    "* Try the cosine similarity or pearson similarity measurement instead of Jaccard\n",
    "* Try item-item collaborative filtering instead of user-user CF\n",
    "* Try to include the baseline estimation model in your collaborative filtering model\n",
    "\n",
    "Besides these, you can also figure out your own solutions how to improve the performance. Write your code in the next cell and print out the RMSE of your new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9db63bc87a43>:24: RuntimeWarning: Mean of empty slice\n",
      "  curr_movie_mean = np.nanmean(prediction_mat[:,i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.50808477 5.04574126 5.58341726 ... 4.12121212 4.12121212 4.12121212]\n",
      " [5.27570171 4.14170901 4.51913205 ... 3.78021978 3.78021978 3.78021978]\n",
      " [3.91666667 3.91666667 3.91666667 ... 3.91666667 3.91666667 3.91666667]\n",
      " ...\n",
      " [4.19625239 4.16666667 4.17283385 ... 4.16666667 4.16666667 4.16666667]\n",
      " [4.03147416 3.84049487 4.08499082 ... 3.84       3.84       3.84      ]\n",
      " [4.03330768 3.68012085 3.89105204 ... 3.62343096 3.62343096 3.62343096]]\n",
      "1.0081794418705394\n"
     ]
    }
   ],
   "source": [
    "# implement your improved model and print out the RMSE\n",
    "# Your Code Here...\n",
    "# i'll be trying user-user cosine similarity measurement instead of jaccard\n",
    "\n",
    "##### converting current ratings using baseline estimate\n",
    "prediction_mat = train_mat.copy()\n",
    "\n",
    "baseline_estimate = get_baseline_estimate(train_mat)\n",
    "\n",
    "prediction_mat = train_mat.copy()\n",
    "num_movies = prediction_mat.shape[1]\n",
    "\n",
    "### calculating cosine similarity matrix\n",
    "numerator = prediction_mat @ prediction_mat.T\n",
    "norm = (prediction_mat * prediction_mat).sum(axis=1, keepdims=True) ** 0.5\n",
    "\n",
    "cosine_sim_mat = numerator / norm /  norm.T\n",
    "\n",
    "\n",
    "\n",
    "num_users = cosine_sim_mat.shape[0]\n",
    "kNearestNeighbors = []\n",
    "for i in range(num_users):\n",
    "    curr_users_neighbors = np.argpartition(cosine_sim_mat[i], -11)[-11:] # getting 11 bc it will include the curr user\n",
    "    curr_users_neighbors = curr_users_neighbors[curr_users_neighbors != i] # removing user himself from neighbors\n",
    "    kNearestNeighbors.append(curr_users_neighbors)\n",
    "\n",
    "\n",
    "prediction_mat = train_mat.copy()\n",
    "nan_prediction_mat = train_mat.copy() # creating a copy so it's easier to do mean calculations\n",
    "nan_prediction_mat[nan_prediction_mat == 0] = np.nan\n",
    "    \n",
    "indicator_mat = (train_mat > 0).astype(float)\n",
    "for i in range(num_users):\n",
    "    curr_users_mean = np.nanmean(nan_prediction_mat[i])\n",
    "    numerator = 0\n",
    "    denom = 0\n",
    "\n",
    "    for j in range(10):\n",
    "        curr_neighbor = kNearestNeighbors[i][j]\n",
    "        curr_neighbor_rating_vec = prediction_mat[curr_neighbor] # gets row of neighbor's ratinggi\n",
    "\n",
    "        nan_curr_neighbor_rating_vec = nan_prediction_mat[curr_neighbor]\n",
    "        curr_neighbors_mean = np.nanmean(curr_neighbor_rating_vec)\n",
    "        if(curr_neighbors_mean == 0):\n",
    "            continue\n",
    "        similarity = cosine_sim_mat[i][kNearestNeighbors[i][j]] # getting the similarity of each of the neighbors\n",
    "        \n",
    "        curr_neighbor_rating_vec[curr_neighbor_rating_vec == 0] = curr_users_mean\n",
    "        rating_dev = curr_neighbor_rating_vec - curr_neighbors_mean\n",
    "        rating_dev *= indicator_mat[curr_neighbor]\n",
    "        numerator += similarity * rating_dev # sets instances where neighbor didn't give a rating to zero\n",
    "        #numerator *= indicator_mat[curr_neighbor]\n",
    "        denom += similarity\n",
    "    \n",
    "    prediction_mat[i] = curr_users_mean +  (numerator / denom)\n",
    "\n",
    "#print(prediction_mat[0])\n",
    "print(prediction_mat)\n",
    "\n",
    "\n",
    "indicator_mat = (test_mat > 0).astype(float)\n",
    "test_rmse = (np.sum(((prediction_mat - test_mat) * indicator_mat) ** 2) / np.sum(indicator_mat)) ** 0.5\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And please briefly explain what your new model does to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model uses the cosine similarity measurement instead of jaccard to improve the performance. This factors in the actual ratings when calculating similarity, whereas jaccard looks only at whether or not two users have rated the same items, discarding the rating given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Recommendations with implicit feedback (40 points total)\n",
    "\n",
    "Now we turn to study how collaborative filtering algorithms work for recommendations with implicit feedback. The overall pipeline is the same as in Part 2 of Homework 1. But now you need to implement a user-user collaborative filtering algorithm for recommendation.\n",
    "\n",
    "In this part, you will use the same MovieLens 1M dataset, and:\n",
    "* write the code to implement a user-user collaborative filtering algorithm,\n",
    "* and evaluate your recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the algorithm implementation, we need to first transfer the explicit datasets you already generated to implicit ones. The process is the same as Part 2a in Homework 1, and the code has been provided. You need to run the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = (train_mat > 0).astype(float)\n",
    "test_mat = (test_mat > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to write code to implement a user-user collaborative filtering algorithm with **cosine similarity**. Although we have not discussed in class how to use collaborative filtering for implicit feedback, it is quite straightforward. \n",
    "\n",
    "* We first need to calculate the cosine similarity between users based on the binary feedback vectors of users; \n",
    "* Then, for a specific user $u$, we predict a preference vector for the user $u$ by weighted averaging the binary feedback vectors of N users who are most similar to $u$;\n",
    "* And last, rank the movies based on the predicted preference vector of the user $u$ as recommendations. \n",
    "\n",
    "The predicted preference score from user $u$ to movie $i$ can be calculated as: $p_{u,i}=\\frac{\\sum_{u^\\prime\\in N}s(u,u^\\prime)r_{u^\\prime,i}}{\\sum_{u^\\prime\\in N}|s(u,u^\\prime)|}$, where $s(u,u^\\prime)$ is the cosine similarity, and we set the size of $N$ as 10.\n",
    "\n",
    "In the next cell, write your code to generate the ranked lists of movies by this user-user collaborative filtering recommendation algorithm for every user, store the result in a numpy array of size (#user, 50), where entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. \n",
    "\n",
    "* Hint: for a user $u$, the movies user $u$ liked in the train_mat should be excluded in the top 50 recommendation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "top5 movies:[104 124  97 113  64]\n",
      "top5 popularity:[1508.2384193  1308.77635792 1242.67568385 1240.04693394 1185.80499485]\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here...\n",
    "user_num = test_mat.shape[0]\n",
    "item_num = test_mat.shape[1]\n",
    "\n",
    "print(train_mat)\n",
    "popularity = np.sum(train_mat, axis=0)\n",
    "\n",
    "prediction_mat = train_mat.copy()\n",
    "\n",
    "numerator = prediction_mat @ prediction_mat.T\n",
    "norm = (prediction_mat * prediction_mat).sum(axis=1, keepdims=True) ** 0.5\n",
    "\n",
    "\n",
    "cosine_mat = numerator / norm /  norm.T\n",
    "#print(similarities)\n",
    "\n",
    "\n",
    "\n",
    "num_users = cosine_mat.shape[0]\n",
    "kNearestNeighbors = []\n",
    "for i in range(num_users):\n",
    "    curr_users_neighbors = np.argpartition(cosine_mat[i], -11)[-11:] # getting 11 bc it will include the curr user\n",
    "    curr_users_neighbors = curr_users_neighbors[curr_users_neighbors != i] # removing user himself from neighbors\n",
    "    kNearestNeighbors.append(curr_users_neighbors)\n",
    "    \n",
    "# getting indices of movies a user has already seen\n",
    "user_train_like = []\n",
    "for u in range(user_num):\n",
    "    user_train_like.append(np.where(train_mat[u, :] > 0)[0])\n",
    "\n",
    "\n",
    "\n",
    "prediction_mat = train_mat.copy()\n",
    "nan_prediction_mat = train_mat.copy() # creating a copy so it's easier to do mean calculations\n",
    "nan_prediction_mat[nan_prediction_mat == 0] = np.nan\n",
    "    \n",
    "indicator_mat = (train_mat > 0).astype(float)\n",
    "for i in range(num_users):\n",
    "    curr_users_mean = np.nanmean(nan_prediction_mat[i])\n",
    "    numerator = 0\n",
    "    denom = 0\n",
    "\n",
    "    for j in range(10):\n",
    "        curr_neighbor = kNearestNeighbors[i][j]\n",
    "        curr_neighbor_rating_vec = prediction_mat[curr_neighbor] # gets row of neighbor's ratinggi\n",
    "\n",
    "        nan_curr_neighbor_rating_vec = nan_prediction_mat[curr_neighbor]\n",
    "        curr_neighbors_mean = np.nanmean(curr_neighbor_rating_vec)\n",
    "        if(curr_neighbors_mean == 0):\n",
    "            continue\n",
    "        similarity = cosine_mat[i][kNearestNeighbors[i][j]] # getting the similarity of each of the neighbors\n",
    "        \n",
    "        curr_neighbor_rating_vec[curr_neighbor_rating_vec == 0] = curr_users_mean\n",
    "        rating_dev = curr_neighbor_rating_vec\n",
    "        rating_dev *= indicator_mat[curr_neighbor]  # sets instances where neighbor didn't give a rating to zero\n",
    "        numerator += similarity * rating_dev\n",
    "        denom += similarity\n",
    "    \n",
    "    prediction_mat[i] = (numerator / denom)\n",
    "\n",
    "    \n",
    "popularity = np.sum(prediction_mat, axis=0)\n",
    "recommendation = []\n",
    "for u in range(user_num):\n",
    "    train_like = user_train_like[u]\n",
    "    scores = popularity.copy()\n",
    "    scores[train_like] = -1\n",
    "    top50_iid = np.argpartition(scores, -50)[-50:]\n",
    "    top50_iid = top50_iid[np.argsort(scores[top50_iid])[-1::-1]]\n",
    "    recommendation.append(top50_iid)\n",
    "recommendation = np.array(recommendation)\n",
    "\n",
    "print('top5 movies:' + str(recommendation[0, :5]))\n",
    "print('top5 popularity:' + str(popularity[recommendation[0, :5]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, you need to evaluate your user-user collaborative filtering algorithm by the held-out testing dataset test_mat for each user. Here, we use the same metrics recall@k and precision@k we used in Part 2c of Homework 1. So you can use the same code here to calculate recall@k and precision@k for k=5, 20, 50 for each user, i.e., six metrics for every user. And please print out the average over all users for these six metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5\t[0.038146],\t||\t recall@20\t[0.109485],\t||\t recall@50\t[0.190705]\n",
      "precision@5\t[0.278576],\t||\t precision@20\t[0.213055],\t||\t precision@50\t[0.157652]\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall@k, precision@k with k=5, 20, 50 and print out the average over all users for these 6 metrics.\n",
    "# Your Code Here...\n",
    "\n",
    "user_test_like = []\n",
    "for u in range(user_num):\n",
    "    user_test_like.append(np.where(test_mat[u, :] > 0)[0])\n",
    "    \n",
    "recalls = np.zeros(3)\n",
    "precisions = np.zeros(3)\n",
    "user_count = 0.\n",
    "\n",
    "for u in range(user_num):\n",
    "    test_like = user_test_like[u]\n",
    "    test_like_num = len(test_like)\n",
    "    if test_like_num == 0:\n",
    "        continue\n",
    "    rec = recommendation[u, :]\n",
    "    hits = np.zeros(3)\n",
    "    for k in range(50):\n",
    "        if rec[k] in test_like:\n",
    "            if k < 50:\n",
    "                hits[2] += 1\n",
    "                if k < 20:\n",
    "                    hits[1] += 1\n",
    "                    if k < 5:\n",
    "                        hits[0] += 1\n",
    "    recalls[0] += (hits[0] / test_like_num)\n",
    "    recalls[1] += (hits[1] / test_like_num)\n",
    "    recalls[2] += (hits[2] / test_like_num)\n",
    "    precisions[0] += (hits[0] / 5.)\n",
    "    precisions[1] += (hits[1] / 20.)\n",
    "    precisions[2] += (hits[2] / 50.)\n",
    "    user_count += 1\n",
    "\n",
    "recalls /= user_count\n",
    "precisions /= user_count\n",
    "\n",
    "print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
    "print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
